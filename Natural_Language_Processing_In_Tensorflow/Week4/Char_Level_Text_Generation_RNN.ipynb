{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char_Level_Text_Generation_RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO82w23Nk7GG6IQUjf2d6Gh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohdtanweer/TensorFlowSpecialisation/blob/master/Natural_Language_Processing_In_Tensorflow/Week4/Char_Level_Text_Generation_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvsFd_63r-MR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtGyKoQtsIgS",
        "colab_type": "text"
      },
      "source": [
        "# Download the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_pTiyqTsF6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sKD5pVZsMfQ",
        "colab_type": "code",
        "outputId": "a19d83b0-d510-4bf4-94de-8a14cd9f6a38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iydfLG5sQZy",
        "colab_type": "code",
        "outputId": "30a5e107-c773-412c-91cc-fac978340630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQernai-sV0H",
        "colab_type": "code",
        "outputId": "c06766b5-d533-408e-f6be-6e639a5e2df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZmKnwIVsaDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEDv-n_Usi0S",
        "colab_type": "code",
        "outputId": "a7b127ec-738c-4ff1-c29a-63e0db5b46e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxQL3GUFsq4M",
        "colab_type": "code",
        "outputId": "df1acbc4-b242-4244-9fb9-e25b0cb9c7a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RJL5BKgs4cL",
        "colab_type": "code",
        "outputId": "2ea6481a-43e0-4830-ab7d-5499db35e0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ItIGiMjtDfh",
        "colab_type": "code",
        "outputId": "33551f83-c10d-4056-edc2-db1da94bf927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNZGdXpStHke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtdY-ajDtLZO",
        "colab_type": "code",
        "outputId": "f9fd30af-5a7d-407b-c3bf-52e315ad3fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVBRdNnmtNxL",
        "colab_type": "code",
        "outputId": "93763d8a-1c85-4576-8c17-e80520c97f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpZUq_65tQ-z",
        "colab_type": "code",
        "outputId": "5d361c22-7eb9-4750-8a7a-87e2a0612f82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o8Q2oCotUCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMParmOztW2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5_S5BwjtZho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI-LsOSithYk",
        "colab_type": "code",
        "outputId": "8944800f-4611-4e71-8713-edf252f246e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "604TUMpCtk_i",
        "colab_type": "code",
        "outputId": "76cbf280-9dca-4751-b89b-029b47f5ca12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTVMSKKatw1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6f4Qfzat1bi",
        "colab_type": "code",
        "outputId": "a4a329b5-c190-4984-a48f-1355cb55daea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'ARD II:\\nYour cares set up do not pluck my cares down.\\nMy care is loss of care, by old care done;\\nYou'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"'gqEAJgk lMz?TukiV.. Ymtz\\nGopImUnVukVjHtKq-oCnKEfC.3k\\n\\nXLst:VmQ.KmjcydKjLNOKTkOUfBKIvd;obmIcewqXZlhT\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrs8p8jHt4pI",
        "colab_type": "code",
        "outputId": "965bd6a5-f0f6-440c-ca21-6c000809f6a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1734123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afrOEx93t-BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pr-wHCXuAa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJw9HWWzuEgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScJKQaRduGMb",
        "colab_type": "code",
        "outputId": "56cab165-4ffc-4be0-f405-e545cf91f228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 2.6862\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 12s 69ms/step - loss: 1.9569\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.6928\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.5446\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.4555\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.3969\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.3504\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.3130\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.2775\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 12s 73ms/step - loss: 1.2444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkuzE-2yuHf2",
        "colab_type": "code",
        "outputId": "1448e7a3-4e1b-4fc2-8391-470eb8f7ce56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAe4san5vzmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiLt0WitwupP",
        "colab_type": "code",
        "outputId": "35862bdb-8e82-4f9d-873a-5aed080155db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI0VWUzlwxAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtQ0iZ87wzBh",
        "colab_type": "code",
        "outputId": "8a19750c-5fcf-4001-a7ca-d3ffb0296511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: what you not am, are thy bed;\n",
            "My father are whereof blood upon him legs shall wat bale born agains the garpe fight\n",
            "Witch his princely gills, pleasatist, so we'll woo healthas grace,\n",
            "Dividing out myself, but my wife.\n",
            "\n",
            "AUFIDIUS:\n",
            "They clay'd thee wom my tent.\n",
            "\n",
            "BICHOPSO:\n",
            "O, let,ing me; ma! I say, do you to this kind.\n",
            "Away?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "\n",
            "JULIET:\n",
            "O Fortuse, Plantagenet!\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "They dread their dut's champie'd assos your worship'st him:\n",
            "Tremoliom, he may be some other man.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "A wholence thou shalt show your queen Rome, let it be,\n",
            "Is lower by prevast.\n",
            "\n",
            "KING RICHARD II:\n",
            "Look but the little; gentle business?\n",
            "\n",
            "SIMan a woman; and the best\n",
            "Which be salean'd,\n",
            "Not liberations.\n",
            "My heart-scalenAng spire beloved; both give thee to suchard,\n",
            "When thou livate this to stay to some cold terful:\n",
            "Where by the fairest-crooks of life: the tempest lives of\n",
            "beseech you, let him death, for being nothings seen,\n",
            "Could om something hath mauseder with the seam:\n",
            "Or a mile; grant thyself an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f50NneSxReAo",
        "colab_type": "text"
      },
      "source": [
        "# Advanced: Customized Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTCCDob8w0j-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwdFK13TRgTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdFnyYjwRhZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2hqpYXdRjOj",
        "colab_type": "code",
        "outputId": "df5eab13-2269-42fe-f5c6-af1dd0ff2f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.174203872680664\n",
            "Epoch 1 Batch 100 Loss 2.3634073734283447\n",
            "Epoch 1 Loss 2.1820\n",
            "Time taken for 1 epoch 13.23409914970398 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1735782623291016\n",
            "Epoch 2 Batch 100 Loss 1.9121414422988892\n",
            "Epoch 2 Loss 1.8315\n",
            "Time taken for 1 epoch 12.243868350982666 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.8598190546035767\n",
            "Epoch 3 Batch 100 Loss 1.6896371841430664\n",
            "Epoch 3 Loss 1.6004\n",
            "Time taken for 1 epoch 12.4209144115448 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5739306211471558\n",
            "Epoch 4 Batch 100 Loss 1.5220364332199097\n",
            "Epoch 4 Loss 1.5109\n",
            "Time taken for 1 epoch 12.41809868812561 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4351860284805298\n",
            "Epoch 5 Batch 100 Loss 1.4590128660202026\n",
            "Epoch 5 Loss 1.4229\n",
            "Time taken for 1 epoch 12.323256492614746 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.380855679512024\n",
            "Epoch 6 Batch 100 Loss 1.4331657886505127\n",
            "Epoch 6 Loss 1.3963\n",
            "Time taken for 1 epoch 12.25675892829895 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.347123622894287\n",
            "Epoch 7 Batch 100 Loss 1.364170789718628\n",
            "Epoch 7 Loss 1.3828\n",
            "Time taken for 1 epoch 12.293692111968994 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2668622732162476\n",
            "Epoch 8 Batch 100 Loss 1.3322237730026245\n",
            "Epoch 8 Loss 1.3227\n",
            "Time taken for 1 epoch 12.31391954421997 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2841829061508179\n",
            "Epoch 9 Batch 100 Loss 1.3306288719177246\n",
            "Epoch 9 Loss 1.3031\n",
            "Time taken for 1 epoch 12.293646812438965 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.1788690090179443\n",
            "Epoch 10 Batch 100 Loss 1.2008881568908691\n",
            "Epoch 10 Loss 1.3036\n",
            "Time taken for 1 epoch 12.347457647323608 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}